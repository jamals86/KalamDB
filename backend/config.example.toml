# KalamDB Server Configuration
# This is an example configuration file with all available settings.
# Copy this file to config.toml and adjust values for your environment.
#
# NOTE: Runtime configuration only!
# - Namespace and storage location configuration is stored in system tables (via kalamdb-sql)
# - This file contains only server runtime settings (ports, paths, limits, etc.)

[server]
# Server bind address (default: 127.0.0.1)
host = "127.0.0.1"

# Server port (default: 8080)
port = 8080

# Number of worker threads (0 = number of CPU cores)
workers = 0

# API version (default: "v1")
# Controls the versioned endpoint prefix (e.g., /v1/api/sql)
api_version = "v1"

[storage]
# RocksDB data directory path (relative or absolute)
rocksdb_path = "./data/rocksdb"

# Default storage path for 'local' storage (when base_directory='')
# Used for table flushes to Parquet files (default: "./data/storage")
default_storage_path = "./data/storage"

# Enable Write-Ahead Log for durability (default: true)
enable_wal = true

# Compression algorithm: none, snappy, zlib, lz4, zstd (default: snappy)
compression = "snappy"

[storage.rocksdb]
# Write buffer size per column family in bytes (default: 64MB)
# Larger values improve write throughput but use more memory
write_buffer_size = 67108864

# Maximum number of write buffers (default: 3)
# Allows writes to continue while buffers are being flushed
max_write_buffers = 3

# Block cache size for reads in bytes (default: 256MB)
# Larger values improve read performance for hot data
block_cache_size = 268435456

# Maximum number of background compaction/flush jobs (default: 4)
max_background_jobs = 4

[datafusion]
# Memory limit for query execution in bytes (default: 1GB)
# Queries exceeding this limit will be terminated
memory_limit = 1073741824

# Number of parallel threads for query execution (default: number of CPU cores)
# Set to 0 to auto-detect CPU count
query_parallelism = 0

# Maximum number of partitions per query (default: 16)
# Higher values increase parallelism but use more resources
max_partitions = 16

# Batch size for record processing (default: 8192 rows)
# Larger batches improve throughput but use more memory
batch_size = 8192

[flush]
# Node ID for SNOWFLAKE_ID() function (default: 0, range: 0-1023)
# Each server instance in a distributed setup should have a unique node_id
# to ensure globally unique Snowflake IDs across nodes
node_id = 0

# Default row limit for flush policies (default: 10000 rows)
# Tables without explicit flush policy will use this value
default_row_limit = 10000

# Default time interval for flush in seconds (default: 300s = 5 minutes)
# Tables will flush to Parquet after this duration
default_time_interval = 300

# T158j: Flush job shutdown timeout in seconds (default: 300s = 5 minutes)
# Server waits this duration for active flush jobs to complete during shutdown
flush_job_shutdown_timeout_seconds = 300

# T158q: Job retention days (default: 30 days)
# Job records older than this are deleted during cleanup
job_retention_days = 30

# T158r: Job cleanup schedule (cron format, default: daily at midnight UTC)
# Format: "second minute hour day_of_month month day_of_week"
# Example: "0 0 * * *" = daily at midnight
job_cleanup_schedule = "0 0 * * *"

[retention]
# Default retention hours for soft-deleted rows (default: 168 hours = 7 days)
# Rows with _deleted=true will be kept in Parquet files for this duration
default_deleted_retention_hours = 168

[stream]
# Default TTL for stream table rows in seconds (default: 10 seconds)
# Stream tables are ephemeral - rows expire after this duration
default_ttl_seconds = 10

# Default maximum buffer size for stream tables (default: 10000 rows)
# Oldest rows are evicted when buffer exceeds this limit
default_max_buffer = 10000

# Stream eviction interval in seconds (default: 60 seconds = 1 minute)
# How often the background task checks and evicts expired events
eviction_interval_seconds = 60

[limits]
# Maximum message size for REST API requests in bytes (default: 1MB)
max_message_size = 1048576

# Maximum rows that can be returned in a single query (default: 1000)
max_query_limit = 1000

# Default LIMIT for queries without explicit LIMIT clause (default: 50)
default_query_limit = 50

[logging]
# Log level: error, warn, info, debug, trace (default: info)
level = "info"

# Log file path (relative or absolute)
file_path = "./logs/app.log"

# Also log to console/stdout (default: true)
log_to_console = true

# Log format: compact, pretty, json (default: compact)
format = "compact"

[performance]
# Request timeout in seconds (default: 30s)
# Requests exceeding this duration will be terminated
request_timeout = 30

# Keep-alive timeout in seconds (default: 75s)
keepalive_timeout = 75

# Maximum concurrent connections (default: 25000)
# Includes both REST API and WebSocket connections
max_connections = 25000

[rate_limit]
# Maximum SQL queries per second per user (default: 100)
# Prevents query flooding from a single user
max_queries_per_sec = 100

# Maximum WebSocket messages per second per connection (default: 50)
# Prevents message flooding on WebSocket connections
max_messages_per_sec = 50

# Maximum concurrent live query subscriptions per user (default: 10)
# Limits total active subscriptions to prevent resource exhaustion
max_subscriptions_per_user = 10
