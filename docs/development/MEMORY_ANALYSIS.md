# KalamDB Server Memory Analysis & Optimization Guide

## Executive Summary

Based on code analysis, KalamDB's ~50MB idle memory footprint comes from these major components:

| Component | Est. Memory | Description |
|-----------|-------------|-------------|
| **RocksDB** | ~15-20 MB | Write buffers, block cache, metadata |
| **DataFusion** | ~10-15 MB | Query engine, catalog, schemas |
| **Raft Consensus** | ~8-12 MB | State machines (33 groups), log buffers |
| **DashMap Caches** | ~5-8 MB | SchemaRegistry, ConnectionsManager indices |
| **System Tables** | ~3-5 MB | 9 EntityStore providers |
| **Arc-wrapped Objects** | ~3-5 MB | Configs, registries, shared state |
| **Tokio Runtime** | ~2-3 MB | Thread pool, async machinery |
| **Misc Libraries** | ~3-5 MB | bcrypt, JWT, HTTP server |

---

## Detailed Memory Breakdown

### 1. **AppContext (Central Hub) - ~5-8 MB**

**Location**: [backend/crates/kalamdb-core/src/app_context.rs](../../backend/crates/kalamdb-core/src/app_context.rs)

**Structure**:
```rust
pub struct AppContext {
    node_id: Arc<NodeId>,                    // ~100 bytes
    config: Arc<ServerConfig>,                // ~2-3 KB (nested configs)
    schema_registry: Arc<SchemaRegistry>,     // ~2-3 MB (DashMap caches)
    user_table_store: Arc<UserTableStore>,    // ~500 KB
    shared_table_store: Arc<SharedTableStore>,// ~500 KB
    storage_backend: Arc<dyn StorageBackend>, // ~300 bytes (trait object)
    job_manager: OnceCell<Arc<JobsManager>>,  // ~1-2 MB
    live_query_manager: OnceCell<Arc<LiveQueryManager>>, // ~1-2 MB
    connection_registry: Arc<ConnectionsManager>, // ~1-2 MB
    storage_registry: Arc<StorageRegistry>,   // ~500 KB
    session_factory: Arc<DataFusionSessionFactory>, // ~1 MB
    base_session_context: Arc<SessionContext>, // ~3-5 MB (DataFusion)
    system_tables: Arc<SystemTablesRegistry>, // ~2-3 MB
    executor: Arc<dyn CommandExecutor>,       // ~8-12 MB (Raft)
    manifest_service: Arc<ManifestService>,   // ~500 KB
    // ... other fields
}
```

**Optimization Opportunities**:
- ‚úÖ Already uses Arc for zero-copy sharing
- ‚ö†Ô∏è OnceCell defers initialization (good)
- üî¥ base_session_context is DataFusion-heavy (see below)

---

### 2. **SchemaRegistry (DashMap Cache) - ~2-3 MB**

**Location**: [backend/crates/kalamdb-core/src/schema_registry/registry/core.rs](../../backend/crates/kalamdb-core/src/schema_registry/registry/core.rs)

**Structure**:
```rust
pub struct SchemaRegistry {
    app_context: OnceLock<Arc<AppContext>>,
    table_cache: DashMap<TableId, Arc<CachedTableData>>,      // ~1-2 MB
    version_cache: DashMap<TableVersionId, Arc<CachedTableData>>, // ~1-2 MB
    base_session_context: OnceLock<Arc<SessionContext>>,
}
```

**Memory per table**:
- `CachedTableData`: ~50-100 KB per table (Arrow schema, provider, metadata)
- Empty DB: ~50 KB (system tables only)
- 100 tables: ~5-10 MB

**Optimization**:
```rust
// Current: Pre-allocates capacity
DashMap::with_capacity(10000) // Wastes memory if unused

// Recommended: Lazy allocation
DashMap::new() // Grows as needed, ~48 bytes overhead
```

---

### 3. **RocksDB Backend - ~15-20 MB**

**Location**: RocksDB native library + Rust wrapper

**Configuration** ([backend/src/lifecycle.rs:62-64](../../backend/src/lifecycle.rs#L62-L64)):
```rust
let backend = Arc::new(RocksDBBackend::with_options(
    db,
    config.storage.rocksdb.sync_writes,  // false = async (faster)
    config.storage.rocksdb.disable_wal,  // false = WAL enabled (safe)
));
```

**Memory breakdown**:
- **Write buffer**: 64 MB default (memtable) ‚Üí configurable via `server.toml`
- **Block cache**: 8 MB default (LRU cache for reads) ‚Üí configurable
- **Metadata**: ~2-3 MB (column families, indexes, bloom filters)
- **WAL buffer**: ~4 MB (write-ahead log)

**Optimization** (server.toml):
```toml
[storage.rocksdb]
write_buffer_size = 16_777_216  # 16 MB instead of 64 MB
max_write_buffer_number = 2     # Reduce from 3 to 2
block_cache_size = 4_194_304    # 4 MB instead of 8 MB
```

**Impact**: Saves ~30-40 MB but may reduce throughput for write-heavy workloads.

---

### 4. **DataFusion Query Engine - ~10-15 MB**

**Location**: [backend/crates/kalamdb-core/src/sql/datafusion_session.rs](../../backend/crates/kalamdb-core/src/sql/datafusion_session.rs)

**Components**:
- **Catalog**: ~2-3 MB (registered namespaces, tables, schemas)
- **SessionState**: ~3-5 MB (optimizer, planner, execution config)
- **Memory pool**: ~5 MB default (query execution buffers)

**Current config** ([backend/crates/kalamdb-configs/src/lib.rs](../../backend/crates/kalamdb-configs/src/lib.rs)):
```rust
pub struct DataFusionConfig {
    pub max_memory: usize,           // 1 GB default
    pub target_partitions: usize,    // CPU cores
    pub batch_size: usize,           // 8192 rows
}
```

**Optimization**:
```toml
[datafusion]
batch_size = 4096              # Reduce from 8192
target_partitions = 2          # Reduce parallelism for idle server
```

**Why it matters**: DataFusion pre-allocates execution resources even when idle.

---

### 5. **Raft Consensus Layer - ~8-12 MB**

**Location**: [backend/crates/kalamdb-raft/src/manager/raft_manager.rs](../../backend/crates/kalamdb-raft/src/manager/raft_manager.rs)

**Structure**:
```rust
pub struct RaftManager {
    node_id: NodeId,
    meta: Arc<RaftGroup<MetaStateMachine>>,           // ~2-3 MB
    user_data_shards: Vec<Arc<RaftGroup<...>>>,       // 32 groups √ó 150 KB = ~4.8 MB
    shared_data_shards: Vec<Arc<RaftGroup<...>>>,     // 1 group √ó 150 KB = ~150 KB
    // Total: 33 Raft groups
}
```

**Per-group memory**:
- Log buffer: ~50 KB (in-memory log entries)
- State machine: ~50-100 KB (metadata, indices)
- Network state: ~20-30 KB (peer connections, heartbeat timers)

**Single-node mode** (standalone):
- Still creates all 33 Raft groups (1 meta + 32 data shards)
- No peers, but overhead remains

**Optimization**:
```toml
[cluster]
user_shards = 8          # Reduce from 32 (default)
shared_shards = 1        # Keep at 1
```

**Trade-off**: Fewer shards = less memory but reduced write parallelism.

---

### 6. **SystemTablesRegistry - ~2-3 MB**

**Location**: [backend/crates/kalamdb-system/src/registry.rs](../../backend/crates/kalamdb-system/src/registry.rs)

**Structure**:
```rust
pub struct SystemTablesRegistry {
    users: Arc<UsersTableProvider>,           // ~300 KB
    jobs: Arc<JobsTableProvider>,             // ~300 KB
    job_nodes: Arc<JobNodesTableProvider>,    // ~300 KB
    namespaces: Arc<NamespacesTableProvider>, // ~300 KB
    storages: Arc<StoragesTableProvider>,     // ~300 KB
    live_queries: Arc<LiveQueriesTableProvider>, // ~300 KB
    tables: Arc<TablesTableProvider>,         // ~300 KB
    audit_logs: Arc<AuditLogsTableProvider>,  // ~300 KB
    manifest: Arc<ManifestTableProvider>,     // ~300 KB
    // 9 providers √ó 300 KB = ~2.7 MB
}
```

**Per-provider overhead**:
- EntityStore wrapper: ~50 KB
- Cached schemas: ~50 KB (Arrow DataType metadata)
- Index metadata: ~100 KB (secondary indices)
- DashMap overhead: ~50-100 KB

**Optimization**: Not much - these are essential system tables.

---

### 7. **ConnectionsManager (WebSocket) - ~1-2 MB**

**Location**: [backend/crates/kalamdb-core/src/live/connections_manager.rs](../../backend/crates/kalamdb-core/src/live/connections_manager.rs)

**Structure**:
```rust
pub struct ConnectionsManager {
    connections: DashMap<ConnectionId, SharedConnectionState>, // ~500 KB
    subscriptions: DashMap<LiveQueryId, SubscriptionHandle>,   // ~500 KB
    table_subscriptions: DashMap<TableId, Vec<LiveQueryId>>,   // ~500 KB
    // Empty DB: ~1.5 MB overhead
}
```

**Per-connection overhead**:
- ConnectionState: ~10 KB (user_id, auth, heartbeat timer)
- Notification channel: ~8 KB (mpsc::channel buffer)
- Flow control: ~2 KB (AtomicBool, buffers)

**Optimization**:
```rust
// Current: Pre-allocated capacity
DashMap::with_capacity(1000)  // Wastes memory

// Recommended: Lazy allocation
DashMap::new()  // ~48 bytes overhead, grows as needed
```

---

### 8. **ManifestService (Parquet Cache) - ~500 KB**

**Location**: [backend/crates/kalamdb-core/src/manifest/service.rs](../../backend/crates/kalamdb-core/src/manifest/service.rs)

**Structure**:
```rust
pub struct ManifestService {
    provider: Arc<ManifestTableProvider>,  // ~200 KB
    config: ManifestCacheSettings,         // ~100 bytes
    schema_registry: Option<Arc<...>>,     // ~8 bytes (pointer)
    storage_registry: Option<Arc<...>>,    // ~8 bytes (pointer)
}
```

**Cache behavior**:
- Lazy loading via `get_or_load()` (no pre-loading)
- Indexed by TableId + UserId
- Eviction via TTI (time-to-idle)

**Optimization**: Already optimal (lazy + indexed + TTL-based).

---

### 9. **Tokio Async Runtime - ~2-3 MB**

**Configuration**: Actix-Web uses multi-threaded Tokio runtime

**Memory breakdown**:
- Thread pool: ~1 MB (worker threads)
- Task queue: ~500 KB (pending futures)
- Timers/IO: ~500 KB (epoll/kqueue buffers)

**Optimization**:
```rust
// Current: Actix default (# of CPU cores)
#[actix_web::main]
async fn main() { ... }

// Recommended: Limit workers for idle server
tokio::runtime::Builder::new_multi_thread()
    .worker_threads(2)  // Reduce from CPU count
    .build()
```

---

## Optimization Recommendations

### üéØ High-Impact (10-20 MB savings)

1. **Reduce RocksDB memory** (saves ~20 MB):
   ```toml
   [storage.rocksdb]
   write_buffer_size = 16_777_216  # 16 MB (was 64 MB)
   block_cache_size = 4_194_304    # 4 MB (was 8 MB)
   max_write_buffer_number = 2     # 2 buffers (was 3)
   ```

2. **Reduce Raft shards** (saves ~5-8 MB):
   ```toml
   [cluster]
   user_shards = 8  # Reduce from 32
   ```

3. **Lazy DashMap allocation** (saves ~2-3 MB):
   ```rust
   // Replace all DashMap::with_capacity(...) with DashMap::new()
   table_cache: DashMap::new(),
   version_cache: DashMap::new(),
   connections: DashMap::new(),
   ```

### ‚öñÔ∏è Medium-Impact (3-5 MB savings)

4. **Reduce DataFusion batch size** (saves ~2-3 MB):
   ```toml
   [datafusion]
   batch_size = 2048  # Reduce from 8192
   ```

5. **Limit Tokio worker threads** (saves ~1-2 MB):
   ```rust
   tokio::runtime::Builder::new_multi_thread()
       .worker_threads(2)
       .build()
   ```

### üî¨ Low-Impact (< 2 MB savings)

6. **OnceCell for deferred init** (already implemented):
   - job_manager, live_query_manager, sql_executor use OnceCell ‚úÖ

7. **Arc for zero-copy sharing** (already implemented):
   - node_id, config, all managers wrapped in Arc ‚úÖ

---

## Memory Profiling Tools

### macOS (current platform)

1. **Activity Monitor**:
   ```bash
   # Real memory usage
   ps aux | grep kalamdb | awk '{print $6/1024 " MB"}'
   ```

2. **Instruments (Xcode)**:
   ```bash
   # Detailed heap analysis
   instruments -t Allocations /path/to/kalamdb
   ```

3. **Valgrind (via Docker)**:
   ```bash
   # Linux container required
   valgrind --tool=massif --pages-as-heap=yes cargo run
   ```

4. **jemalloc profiling**:
   ```toml
   # Add to Cargo.toml
   [dependencies]
   jemalloc-ctl = "0.5"
   tikv-jemallocator = "0.5"
   ```

---

## Implementation Status

### ‚úÖ Phase 1: Quick Wins (COMPLETED)
- [x] Changed `DashMap::with_capacity(1024)` to `DashMap::new()` in ConnectionsManager
- [x] Updated `server.toml` with reduced RocksDB settings:
  - write_buffer_size: 4MB ‚Üí 2MB
  - block_cache_size: 16MB ‚Üí 4MB
  - max_write_buffers: 2 (kept)
- [x] Reduced Raft shards: user_shards = 32 ‚Üí 8
- [x] Reduced DataFusion batch_size: 4096 ‚Üí 2048
- [x] Updated default values in `kalamdb-configs`:
  - `defaults.rs`: RocksDB (2MB/4MB), DataFusion (2048), Raft (8 shards)
  - `cluster.rs`: default_user_shards() = 8
  - `types.rs`: Updated struct comments
- [x] Updated `server.example.toml` with optimized defaults
- [x] Added documentation about RocksDB shared block cache (memory efficient!)

**Key Insight - RocksDB Block Cache**:
The block cache is **SHARED** across all column families! Adding 100 tables with a 4MB cache still only uses 4MB total (not 400MB). This is critical for multi-tenant databases.

**Files Modified**:
1. `backend/crates/kalamdb-core/src/live/connections_manager.rs` - Lazy DashMap allocation
2. `backend/server.toml` - Memory-optimized config settings  
3. `backend/server.example.toml` - Memory-optimized defaults with documentation
4. `backend/crates/kalamdb-configs/src/config/defaults.rs` - Updated default functions
5. `backend/crates/kalamdb-configs/src/config/cluster.rs` - Reduced user_shards default
6. `backend/crates/kalamdb-configs/src/config/types.rs` - Updated struct comments
7. `backend/crates/kalamdb-store/src/rocksdb_init.rs` - Added shared cache documentation

### üîÑ Phase 2: Testing & Validation (TODO)
- [ ] Run smoke tests to ensure no regressions
- [ ] Measure actual memory reduction (before: ~50MB, expected after: ~35-40MB)
- [ ] Performance benchmarks (write throughput, query latency)

### üìã Phase 3: Advanced Optimizations (FUTURE)
- [ ] Implement jemalloc profiler integration
- [ ] Add `/metrics/memory` endpoint for live monitoring
- [ ] Profile with Instruments on macOS
- [ ] Create memory regression tests

---

## Expected Results

| Optimization | Memory Savings | Performance Impact | Status |
|--------------|----------------|-------------------|---------|
| RocksDB tuning | ~10 MB | Moderate write slowdown | ‚úÖ DONE |
| Lazy DashMap | ~1-2 MB | None (lazy allocation) | ‚úÖ DONE |
| Raft shards | ~5 MB | Reduced write parallelism | ‚úÖ DONE |
| DataFusion batch | ~1-2 MB | Slightly slower queries | ‚úÖ DONE |
| Tokio workers | ~1 MB | Lower throughput ceiling | ‚è≥ TODO |
| **TOTAL** | **~18-20 MB** | **Acceptable for idle server** | |

**Target footprint**: ~30-35 MB idle (was ~50 MB)

**To verify the optimizations**:
```bash
# Start the server
cd backend && cargo run

# In another terminal, check memory usage
ps aux | grep kalamdb | grep -v grep | awk '{print $6/1024 " MB"}'

# Or use Activity Monitor on macOS
```

---

## Notes

- **Arc overhead**: Each Arc adds 16 bytes (pointer + ref counts)
- **DashMap overhead**: ~48 bytes + 16 bytes per shard (lock-free)
- **OnceCell overhead**: ~24 bytes (state + pointer)
- **SystemColumnsService**: Uses Snowflake ID generator (~500 bytes)

**Last updated**: January 26, 2026
**Baseline**: 50 MB idle memory (fresh server, no tables)

---

## Additional Optimization Opportunities

### 1. **OnceCell/Lazy Initialization (Already Implemented ‚úÖ)**
The codebase already uses `OnceCell` for deferred initialization:
- `job_manager`, `live_query_manager`, `sql_executor`, `applier` in AppContext
- These are only allocated when first accessed, not at startup

### 2. **RocksDB Shared Block Cache (Already Optimal ‚úÖ)**
The block cache is **shared across ALL column families**:
- 100 tables with 4MB cache = 4MB total (not 400MB!)
- This is a critical optimization already in place
- Adding more tables/CFs increases write buffer memory but NOT cache memory

### 3. **Future: Conditional Feature Compilation**
For specialized deployments, consider feature flags:
```toml
[features]
default = ["websockets", "raft", "auth"]
minimal = [] # No WebSockets, no Raft, no auth
websockets = ["actix-web-actors", "tokio/sync"]
raft = ["openraft", "kalamdb-raft"]
```
**Impact**: Could save ~10-15 MB for read-only/embedded use cases

### 4. **Future: Tokio Runtime Tuning**
Currently using Actix-Web's default (all CPU cores):
```rust
// Current (implicit)
#[actix_web::main]
async fn main() { ... }

// Optimized for idle server
tokio::runtime::Builder::new_multi_thread()
    .worker_threads(2)  // Reduce from CPU count
    .max_blocking_threads(4)  // Reduce from 512
    .build()
```
**Impact**: ~1-2 MB savings, lower throughput ceiling

### 5. **Future: Arrow Schema Deduplication**
Multiple tables with identical schemas could share `Arc<Schema>`:
```rust
// Schema cache with deduplication
let schema_cache: DashMap<u64, Arc<Schema>> = DashMap::new();
let schema_hash = hash_schema_definition(fields);
schema_cache.entry(schema_hash).or_insert_with(|| Arc::new(schema))
```
**Impact**: ~100-500 KB per 100 duplicate schemas

### 6. **Future: Manifest Hot Cache Tuning**
Current default: 1000 entries (was 50,000)
```rust
// defaults.rs
pub fn default_manifest_cache_max_entries() -> usize {
    1000 // Already reduced from 50,000
}
```
**Status**: Already optimized ‚úÖ

### 7. **Memory Profiling Integration**
Add jemalloc heap profiler for live monitoring:
```toml
[dependencies]
tikv-jemallocator = { version = "0.5", optional = true }
jemalloc-ctl = { version = "0.5", optional = true }

[features]
jemalloc = ["tikv-jemallocator", "jemalloc-ctl"]
```
**Use**: `MALLOC_CONF=prof:true cargo run --features jemalloc`
