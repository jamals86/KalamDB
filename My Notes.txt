Things to add to the spec:
- Include TODO's found in the codebase, maybe some needs to be added to the plan and tasks
- Add support for parametrized queries from the client /api/sql which then will improve the query planning/optimization
    - Example for this will be: SELECT * FROM messages WHERE user_id = $1 AND created_at > $2
    - The api should access a sql with: { "sql": "SELECT * FROM system.storage_locations WHERE last_updated = $1","params": ["jamal-saad-4442"]}
    - In the backend datafusion shall compile it once and use it from the cached one
    - Registering tables from user's session - Improve this by also keeping some tables in a cache in memory maybe a user will keep query his own table for a period of time, in this way we can manage this to free cached tables whenever they stop being used
- Add automatic flushing triggering in the system, with how it's planned to be
- Make sure storage path for parquet files should use a templating of a path with parameters: {storageLocation}, {userId}, {shard}, {tableName}, {namespace} this is configured by the user, the shard is a function which the user can set, currently it can be defined by the system with a to z for now

- Implement manual flushing of a table (Will also be used when shutting down the server for backup or maintanance) using SQL command
- I could create a user table in a namespace not found, before creating a user/shared/stream tables the namespace should always exists, its not only an alias here
- Update all crates/libraries to the latest versions
- Rename the system.storage_locations to system.storages only, rename this in everywhere
- System tables providers which is located in: kalamdb-core\src\tables\system, has similar structure, i prefer having a parent class for all of them and impelemnts the same logic one time only
- Re-write the README it has things outdated now
    - Remove the parquet mentioning that much it can be mentioned only once max
    - Add also mention that the websocket is directly opened to the server no need for another service in the middle
- Add also branch and revision from git to the version when running the server prompt
- Log the size of the rocksdb database
- RocksDB folder should be configured using the config .toml file and also by default it need to sit next to the binary file and not temporary table
- Make sure when using scan() we are using it with a key, for erxample scanning the streams should be done only for specific user_id
    - Also scanning user's table should be done with a user_id
    - Stream table as well
- Add code documentation on scan() functions since they are essential for our design
- Check the logic where we query user's table, currently it register the table first and then query it or un-register it, check the workflow here and try to improve performance and the session initialization
- make tests run either on local server or on temporary server like its doing now
- Double check if we can use kalamdb-store with kalamdb-sql instead of calling rocksdb directly from kalamdb-sql
- Check if ddls can be included inside kalamdb-sql as well since they belong there, if it's not all maybe the names or models only
- Default catalog for system tables is system, no need for a different one
- instead of relying on rocksdb with features like column-families, make the project generic so we can add any
    type of storage like sled/redis or anything else
- Maybe its better to have a crate like kalamdb-commons which will be included in all other crates for (Also add this to the requirements to always keep this in mind):
    - models like user_id/namespace_id/..
    - ids
    - system helpers
    - System tables names instead of having multiple places where we name them
    - Any system things that is shared between or needed in -sql/-store/-core
    - Also kalamdb-server configs which all other crates might need their models as well
    - Errors like: backend/crates/kalamdb-core/src/error.rs
- Inside backend\crates\kalamdb-sql\src\models.rs use the same namespace_id and table_id there instead of string's
- Make sure the user can't listen to a shared table since this will ruin performance
- Add a new config which the amdin can choose if to allow localhost to query the database without auth, and also which userId will be used from localhost by default it will be system
    Other hostnames must specify a valid jwt with inside it the userid included
- Add crate for kalamdb-live which is responsible for live subscribers logic and it's communicate with the kalamdb-store and kalamdb-sql
- Instead of having something like this: let cf_name = format!("user_table:{}:{}", namespace_id, table_name); let cf_name = format!("shared_table:{}:{}", namespace_id, table_name);
    make this a helper class which deals with naming, for system/shared/user
- "_deleted" and "_updated" should be stored one time only, currently we are always writing them as string's
- Make sure we use the actor model for the flush jobs so we can view them easily just like live queries/streams/backup jobs as well
- Make sure for expr check like in live_queries we use datafusion expressions and cache them
- For sql functions we can use the datafusion own functions as well
- Make sure when we select/insert/delet/update user table if we specify FROM namespace1.user.user_files then it will be like: FROM namespace1.jamal86.user_files
    Specify .user. should have X-USER-ID in the header if not then there should be an error
    Instead of from("system") we should pass the exact user_id
- ShowBackupStatement for example has the same str: "SHOW BACKUP FOR DATABASE" multiple times, change the code to have it as an enum one time
- In API query result, also return the time took for the query to process, this need to be an option in the configuration toml
- Whenever flushing to parquet store also a column in the parquet
    that indicate which schema version is this
    Check what other columns i need when writing to parquet file 
    or if it's better to indicate this in the file itself, or if there ability
    to have attributes or properties inside the parquet itself
- An option when creating a table to have an index for a specific column
    Create statement should include that index with it as well for example this query:
    CREATE USER TABLE messages (
        id BIGINT,
        conversation_id STRING,
        content STRING,
        INDEX (conversation_id) USING BLOOM (fpp = 0.01, num_items = 10000),
        INDEX (created_at) USING SORTED ASC
    );

- In buffer we also add an index for the user_id for the buffered data, in order to have queries faster
    Also we cna implement indexes as well, but now user_id is enough, so that:
    1) each table will have another column_family
    2) this will be managed with data added in the main table streams with a reference user_id -> refId
    NOTE: Check if this really needed since we have the key with userId already
- Can you check why in kalamdb-store we are not using the same NamespaceId and TableName models for something like this for example:
        namespace_id: &str,
        table_name: &str,
        This will have something consistent and if in the future we want to pass anything beyond we csna do it simply
- Check both user_table_store and shared_table_store and maybe the streams store and also the system tables
    and check if there anything we can improve with polymorphisim
    to write less code there, since all of them looks similar
- Check if there is any crates included is not needed anymore the point here to lower the binary size
- Check that libraries used for testing is not included in released binary
- Create an examples folder in the main repo and add to it a sample typescript project which is a TODO App
    - Add note
    - Delete note
    - Update note
    - Subscribe to the notes changes from the server
    - Add them into local storage for later
- Support when creating a table to have auto-increment syntax, so that whenever the value is empty then 
- Both: kalamdb-core\src\tables\shared_table_provider.rs and kalamdb-core\src\tables\user_table_provider.rs have so many similarities like validation of insert columns combine them both into one validator
    - Maybe it's better to move the validation to be inside kalamdb-sql since it belong there
- Make sure that system table names are defined one time and used instead of each place we write them again as a string, maybe it's better to have one enum for all of them

Features:
- Add CLI tool which is using the API and you can login with specific user to do the actions inside of it, this should be a separate binary
    - This CLI Tool should use the kalamdb-client-sdk which is written in rust and will be compiled as webassembly
    - The client-sdk should also support query caching in the future (So instead than sending sql queries we can send object to prevent always parsing it)
    - For now we can only support typescript-sdk only
- Make the project has a generic serdes models, so we can change the serialization from the json
- A new table called user_files which is a table per user for storing files and their references in the storage location we have
- KFlows - Add a workflow triggers which also listen to streams, actions being done and write to the database
- KFlows will be used for system maintainance like: replication/backup and also periodict cleanup of deleted rows
- Sharding the user's own table's into multiple column families, for example: userA will be redirected to Shard A -> with users.shard-a.messages
    If we have like 5 user tables and 10 shards in total we will have 50 column families
- Raft Cluster - System tables should be replicated to all nodes
- Raft Cluster - User tables each shard should have a group in the cluster and replicated accordingly
- User can easily download his data
- compaction jobs that runs per user whenever he has many parquet files in the system
- Add an optimizre which runs in the background and check if the queries/design is all correctly functioning like:
    - If a table is a shared table and has many rows
    - If a user table is getting big
    - more will come later, the archeticture should support this


Tools & Utilities:
- Build as a docker container Dokerfile
- An example of running with docker-compose
- Auto-deploy to github as an executables
- 