# Data Model

**Feature**: Chat and AI Message History Storage System  
**Date**: 2025-10-13  
**Status**: Phase 1 Design

## Overview

KalamDb's data model is designed for efficient storage, querying, and streaming of chat and AI message history. The model enforces user-centric data ownership with isolation at the storage layer, supports flexible metadata, and optimizes for both real-time writes and historical queries.

---

## Core Entities

### 1. Message

Represents a single message in a conversation (chat message, AI response, system notification).

**Storage**: 
- **RocksDB** (write buffer): Key = `{userId}:{msgId}`, Value = MessageProto (Protobuf or bincode-serialized)
- **Parquet** (consolidated storage): Row in `<userId>/batch-<timestamp>-<index>.parquet`

**Schema**:

| Field | Type | Description | Constraints | Index |
|-------|------|-------------|-------------|-------|
| `msgId` | i64 | Snowflake ID (time-ordered unique identifier) | NOT NULL, PRIMARY KEY | Sorted (implicit in Parquet) |
| `conversationId` | String | Conversation this message belongs to | NOT NULL | Indexed (row group) |
| `from` | String | Sender userId (can be AI, user, or another user) | NOT NULL | - |
| `timestamp` | i64 | Unix timestamp in microseconds (UTC) | NOT NULL | Sorted (via msgId) |
| `content` | String | Message body/text | NOT NULL, MAX 1MB (configurable) | - |
| `metadata` | String (JSON) | Flexible key-value pairs (role, model, tokens, etc.) | NULLABLE | - |

**Parquet Schema (Arrow)**:
```rust
Schema::new(vec![
    Field::new("msgId", DataType::Int64, false),
    Field::new("conversationId", DataType::Utf8, false),
    Field::new("from", DataType::Utf8, false),
    Field::new("timestamp", DataType::Int64, false),
    Field::new("content", DataType::Utf8, false),
    Field::new("metadata", DataType::Utf8, true), // JSON string
])
```

**Validation Rules**:
- `msgId`: Must be valid snowflake ID (generated by server)
- `conversationId`: Non-empty string, max 255 chars
- `from`: Non-empty string, max 255 chars
- `timestamp`: Must be <= current time (server validates, rejects future timestamps)
- `content`: Non-empty, max size enforced by config (default 1MB)
- `metadata`: Valid JSON if present

**Lifecycle**:
1. **Write**: Message written to RocksDB (WAL), acknowledged in <100ms
2. **Consolidation**: Background task reads RocksDB, writes to Parquet (every 5 min or 10k messages)
3. **Delete from RocksDB**: After consolidation, remove from RocksDB to free memory
4. **Query**: DataFusion reads Parquet files, filters by userId/conversationId/msgId range

---

### 2. Conversation

Represents a conversation thread containing multiple messages. Each user has their own set of conversations.

**Storage**:
- **RocksDB** (metadata cache): Key = `{userId}:conversations:{conversationId}`, Value = ConversationMetadata
- **Separate metadata table** (future: SQLite or Parquet index file for fast lookups)

**Schema**:

| Field | Type | Description | Constraints |
|-------|------|-------------|-------------|
| `userId` | String | Owner of this conversation | NOT NULL, PARTITION KEY |
| `conversationId` | String | Unique conversation identifier | NOT NULL, PRIMARY KEY |
| `firstMsgId` | i64 | Snowflake ID of first message | NOT NULL |
| `lastMsgId` | i64 | Snowflake ID of most recent message | NOT NULL, updated periodically |
| `created` | i64 | Unix timestamp when conversation created (microseconds) | NOT NULL |
| `updated` | i64 | Unix timestamp when conversation last modified (microseconds) | NOT NULL |

**Validation Rules**:
- `conversationId`: Non-empty, max 255 chars, unique per user
- `firstMsgId` <= `lastMsgId`
- `created` <= `updated`

**Lifecycle**:
1. **Create**: When first message in conversation arrives, create metadata entry
2. **Update**: Periodically during consolidation, update `lastMsgId` and `updated` timestamp
3. **Query**: List conversations via RocksDB scan of `{userId}:conversations:*` keys

**State Transitions**:
```
[Initial] --first message--> [Active]
[Active] --new message--> [Active] (update lastMsgId)
[Active] --query--> [Active] (read-only)
```

---

### 3. Subscription

Represents an active WebSocket connection subscribing to real-time message updates.

**Storage**:
- **In-memory** (not persisted): HashMap<userId, Vec<SubscriptionHandle>>

**Schema**:

| Field | Type | Description |
|-------|------|-------------|
| `connectionId` | String (UUID) | Unique WebSocket connection identifier |
| `userId` | String | User whose messages this subscription receives |
| `conversationId` | Option<String> | Optional: filter to specific conversation |
| `lastMsgId` | Option<i64> | Optional: only receive messages > this ID |
| `connectedAt` | i64 | Unix timestamp when subscription established |
| `lastHeartbeat` | i64 | Unix timestamp of last ping/pong |
| `websocket` | WebSocket (Actix) | Active WebSocket connection handle |

**Validation Rules**:
- `userId`: Must match authenticated JWT token subject
- `conversationId`: If present, must exist for this user
- `lastMsgId`: If present, replay messages > this ID before subscribing to live stream

**Lifecycle**:
1. **Subscribe**: Client sends WebSocket subscribe message, server creates Subscription entry
2. **Replay**: If `lastMsgId` provided, query Parquet/RocksDB for messages > lastMsgId, send to client
3. **Live Stream**: New messages matching userId/conversationId forwarded to WebSocket
4. **Heartbeat**: Client sends ping every 30s, server updates `lastHeartbeat`
5. **Unsubscribe**: Client disconnects, server removes Subscription entry

---

### 4. User

Logical entity representing a user whose messages are isolated in separate storage partitions.

**Storage**:
- **Implicit**: User is partition key, not stored as separate entity
- **Directory structure**: `<storage-root>/<userId>/batch-*.parquet`

**Schema** (logical, not persisted):

| Field | Type | Description |
|-------|------|-------------|
| `userId` | String | Unique user identifier (from JWT token) |
| `totalMessages` | i64 | Total messages for this user (computed from Parquet + RocksDB) |
| `storageBytes` | i64 | Total storage size for this user (computed from file sizes) |
| `conversations` | Vec<Conversation> | List of conversations for this user |

**Validation Rules**:
- `userId`: Non-empty, max 255 chars, alphanumeric + hyphens/underscores

**Lifecycle**:
- **Create**: Implicitly created when first message for userId arrives
- **Delete**: (Future) Delete all Parquet files and RocksDB entries for userId

---

## Relationships

```
User (1) ----< (N) Conversation
Conversation (1) ----< (N) Message
User (1) ----< (N) Subscription
```

**Cardinality**:
- One User has many Conversations (1:N)
- One Conversation has many Messages (1:N)
- One User has many active Subscriptions (1:N)

**Referential Integrity**:
- Messages reference Conversation via `conversationId` (soft reference, no FK constraint)
- Subscriptions reference User via `userId` (validated against JWT token)

---

## Data Flow

### Write Path
```
Client --> REST API (POST /messages)
       --> Validate JWT + message
       --> Generate snowflake msgId
       --> Write to RocksDB ({userId}:{msgId} = Message)
       --> Acknowledge (return msgId)
       --> Broadcast to active Subscriptions (WebSocket)
```

### Consolidation Path
```
Background Task (every 5 min or 10k messages)
       --> For each user exceeding threshold:
           --> Read messages from RocksDB
           --> Group by conversationId
           --> Write Parquet file ({userId}/batch-{ts}-{idx}.parquet)
           --> Delete from RocksDB
           --> Update conversation metadata (lastMsgId)
```

### Query Path
```
Client --> REST API (GET /messages?userId=X&conversationId=Y)
       --> Validate JWT
       --> DataFusion: SELECT * FROM parquet_files WHERE conversationId = Y
       --> Filter row groups via metadata
       --> Merge with RocksDB (recent messages)
       --> Return paginated results
```

### Subscription Path
```
Client --> WebSocket (subscribe message)
       --> Validate JWT
       --> Create Subscription in memory
       --> If lastMsgId provided:
           --> Query messages > lastMsgId (replay)
           --> Send replay messages to WebSocket
       --> Subscribe to live stream
       --> Forward new messages matching userId/conversationId
```

---

## Indexes & Optimization

### Parquet Row Groups
- **Primary sort**: `msgId` (implicit, snowflake IDs are time-ordered)
- **Row group organization**: Messages grouped by `conversationId`
  - Enables DataFusion to skip row groups when filtering by `conversationId`
- **Row group size**: Target 1MB per row group (~1000 messages @ 1KB/message)

### RocksDB Key Design
- **Message keys**: `{userId}:{msgId}` (8-byte prefix + 8-byte msgId)
  - Enables prefix scan for all messages by userId
- **Conversation keys**: `{userId}:conversations:{conversationId}`
  - Separate namespace for conversation metadata
- **Column families**: Single default CF (keep it simple)

### Bloom Filters
- RocksDB bloom filters enabled (10 bits per key)
- Reduces read amplification during subscription queries

---

## Schema Evolution

### Future Extensions

1. **Custom Columns** (Phase 2+):
   - Allow users to define custom Parquet columns beyond base schema
   - Store custom column definitions in conversation metadata
   - Use Arrow schema merging to combine base + custom columns

2. **Encryption** (Phase 2+):
   - Add `encryptedContent` column (BLOB)
   - Store encryption metadata in `metadata` field (key ID, algorithm)
   - Decrypt on query using user-provided key

3. **Attachments** (Phase 2+):
   - Store attachment URLs/references in `metadata` field
   - Separate storage for large files (S3 with signed URLs)

4. **Message Edits/Deletes** (Phase 2+):
   - Add `editHistory` array column (list of previous versions)
   - Add `deletedAt` timestamp (soft delete)
   - Parquet files immutable; edits stored in separate delta files

---

## Data Retention & Archival

### Retention Policy (configurable)
- **Default**: Messages stored indefinitely
- **Optional**: Delete messages older than N days
  - Implementation: Background task deletes old Parquet files
  - Metadata updated to reflect retention

### Archival Strategy
- **Cold storage**: Move old Parquet files to S3 Glacier (future)
- **Hot storage**: Recent messages (last 30 days) in local/S3 standard
- **Query optimization**: DataFusion skips archived files unless explicitly requested

---

## Data Consistency

### Eventual Consistency Model
- **Writes**: Acknowledged immediately after RocksDB write
- **Consolidation**: Async background process (5 min lag)
- **Queries**: May return stale `lastMsgId` in conversation metadata (updated periodically)

### Consistency Guarantees
- **Write durability**: Messages persist in RocksDB WAL (survives crash)
- **Read-after-write**: Recent messages read from RocksDB + Parquet (merge view)
- **Subscription replay**: Guaranteed delivery of all messages > lastMsgId (no gaps)

### Conflict Resolution
- **Concurrent writes**: RocksDB handles atomicity (single-threaded write path)
- **Duplicate msgIds**: Impossible (snowflake IDs are unique)
- **Consolidation conflicts**: Per-user locking prevents concurrent consolidation

---

## Storage Estimates

### Single Message Size
- **RocksDB**: ~1.2KB (msgId + conversationId + from + timestamp + content + metadata + overhead)
- **Parquet**: ~0.4KB (3x compression via columnar format + Snappy)

### Storage Growth
- **1M messages**: ~400MB Parquet, ~1.2GB RocksDB (before consolidation)
- **100M messages**: ~40GB Parquet, ~120GB RocksDB (unrealistic, consolidation runs continuously)

### Typical Workload (1000 active users, 100 messages/day each)
- **Daily writes**: 100k messages
- **Daily Parquet growth**: ~40MB
- **Annual storage**: ~14.6GB Parquet

---

## Summary

KalamDb's data model prioritizes:
1. **User-centric isolation**: Data partitioned by userId at storage layer
2. **Fast writes**: RocksDB with WAL, <1ms latency
3. **Efficient queries**: Parquet with conversationId row groups, DataFusion SQL
4. **Real-time streaming**: In-memory subscriptions with WebSocket
5. **Flexible metadata**: JSON metadata field for extensibility

**Next Phase**: Define API contracts (REST + WebSocket endpoints)
