# Raft Replication Implementation Tasks

## Overview

This document tracks the implementation of Raft-based replication for KalamDB, enabling multi-node clusters with strong consistency for metadata and data operations.

**Target Architecture:**
- **36 Raft Groups**: 3 metadata + 32 user data shards + 1 shared data shard
- **CommandExecutor Pattern**: Generic abstraction (no if/else for cluster vs standalone)
- **Leader-Only Jobs**: Background jobs (flush, compaction) run only on leader
- **Live Query Sharding**: `system.live_queries` in user shards by `user_id`

---

## Phase 0: Foundation (Dependencies & Crate Setup)
**Status: âœ… Complete**

### Task 0.1: Add Raft Dependencies to Workspace
- [x] Add `openraft` to workspace Cargo.toml (v0.9.21)
- [x] Add `tonic` (gRPC) for network layer
- [x] Add `prost` for protobuf serialization

### Task 0.2: Create `kalamdb-raft` Crate
- [x] Create crate structure under `backend/crates/kalamdb-raft/`
- [x] Add Cargo.toml with workspace dependencies
- [x] Create basic module structure:
  ```
  kalamdb-raft/src/
  â”œâ”€â”€ lib.rs
  â”œâ”€â”€ config.rs          # RaftConfig, ClusterConfig, ShardingConfig
  â”œâ”€â”€ group_id.rs        # GroupId enum (36 groups)
  â”œâ”€â”€ error.rs           # RaftError types
  â”œâ”€â”€ commands/          # Command/Response enums
  â”‚   â”œâ”€â”€ mod.rs
  â”‚   â”œâ”€â”€ system.rs
  â”‚   â”œâ”€â”€ users.rs
  â”‚   â”œâ”€â”€ jobs.rs
  â”‚   â””â”€â”€ data.rs
  â””â”€â”€ executor/          # CommandExecutor trait + DirectExecutor
      â”œâ”€â”€ mod.rs
      â”œâ”€â”€ trait_def.rs
      â””â”€â”€ direct.rs
  ```

### Task 0.3: Define GroupId Enum
- [x] Create GroupId enum with 36 variants (MetaSystem, MetaUsers, MetaJobs, DataUserShard(0-31), DataSharedShard(0))
- [x] Implement shard routing helpers
- [x] Add `ShardRouter` for user_id â†’ shard mapping

---

## Phase 1: CommandExecutor Abstraction
**Status: âœ… Complete**

### Task 1.1: Define CommandExecutor Trait
- [x] Create `CommandExecutor` trait in `kalamdb-raft`
- [x] Define command enums: `SystemCommand`, `UsersCommand`, `JobsCommand`, `UserDataCommand`, `SharedDataCommand`
- [x] Define response enums

### Task 1.2: Implement DirectExecutor (Standalone)
- [x] Create `DirectExecutor` that logs commands (stub implementation)
- [x] Zero overhead for standalone mode

### Task 1.3: Implement StandaloneExecutor (Wired)
- [x] Create `StandaloneExecutor` in `kalamdb-core` that wires to actual providers
- [x] Wire namespace/table/storage operations to SystemTablesRegistry
- [x] Stub user/job/live_query operations (TODO: model adapters)
- [x] Add executor module to kalamdb-core

### Task 1.4: Stub RaftExecutor
- [x] Create `RaftExecutor` struct (placeholder)
- [x] Returns Internal error with "stub" message
- [x] Test for error handling

### Task 1.5: Integrate with AppContext
- [x] Add `executor: Arc<dyn CommandExecutor>` to AppContext
- [x] Add `ClusterSettings` to ServerConfig (kalamdb-commons)
- [x] Factory logic: StandaloneExecutor (no cluster config) vs RaftExecutor (cluster config present)
- [x] Add `executor()` accessor method to AppContext
- [x] Add `is_cluster_mode()` helper method
- [x] Handlers can use `ctx.executor().execute_*()`

---

## Phase 2: State Machine Implementations
**Status: âœ… Complete**

### Task 2.1: KalamStateMachine Trait
- [x] Define common trait for all state machines
- [x] Include apply, snapshot, restore methods
- [x] Idempotency tracking (last_applied LogId)
- [x] bincode 2.x serde helpers (encode/decode)

### Task 2.2: SystemStateMachine
- [x] Handles namespaces, tables, storages metadata
- [x] Snapshot cache with RwLock
- [x] Snapshot/restore implementation

### Task 2.3: UsersStateMachine
- [x] Handles user CRUD operations
- [x] CreateUser, UpdateUser, DeleteUser, RecordLogin, SetLocked
- [x] Snapshot/restore implementation

### Task 2.4: JobsStateMachine
- [x] Handles job creation, status updates
- [x] Includes ClaimJob/ReleaseJob for leader-only execution
- [x] CreateSchedule/DeleteSchedule for scheduled jobs
- [x] Snapshot/restore implementation

### Task 2.5: UserDataStateMachine
- [x] Handles user table INSERT/UPDATE/DELETE
- [x] Handles live_queries for users in this shard
- [x] Routes by user_id % 32 (shard parameter)
- [x] RegisterLiveQuery, UnregisterLiveQuery, CleanupNodeSubscriptions, PingLiveQuery

### Task 2.6: SharedDataStateMachine
- [x] Handles shared table operations
- [x] Single shard for Phase 1
- [x] Tracks recent operations for metrics

---

## Phase 3: Raft Core Implementation
**Status: âœ… Complete**

### Task 3.1: KalamRaftStorage (Combined Storage)
- [x] Implement `openraft::RaftStorage` (v1 API) combining log + state machine
- [x] In-memory log with BTreeMap (index â†’ LogEntryData)
- [x] Vote, commit, purge operations
- [x] Use `Adaptor` to split into RaftLogStorage + RaftStateMachine
- [x] Snapshot creation and restoration

### Task 3.2: gRPC Network Layer
- [x] Create `RaftNetwork` implementing `openraft::RaftNetwork`
- [x] Create `RaftNetworkFactory` for creating connections
- [x] AppendEntries, Vote, InstallSnapshot RPCs (via RPC placeholders)
- [x] Peer registration and node tracking

### Task 3.3: RaftManager
- [x] Orchestrate all 36 Raft groups
- [x] `start()` method to initialize all groups
- [x] `initialize_cluster()` for single-node bootstrap
- [x] `add_node()` for cluster expansion
- [x] `propose_*()` methods routing to correct group
- [x] `is_leader()` and `current_leader()` per group

### Task 3.4: Complete RaftExecutor
- [x] Wire to RaftManager
- [x] Serialize commands with bincode (serde mode)
- [x] Route to correct group via manager
- [x] `is_leader()` and `get_leader()` implementations

---

## Phase 4: Leader-Only Job Execution
**Status: ðŸ”´ Not Started**

### Task 4.1: LeaderOnlyJobExecutor
- [ ] Check `is_leader(GroupId::MetaJobs)` before running jobs
- [ ] Job claiming via Raft (ClaimJob command)
- [ ] node_id tracking in system.jobs

### Task 4.2: Leader Failover Handling
- [ ] Detect orphaned jobs from failed leader
- [ ] Re-queue or fail based on job type
- [ ] `on_become_leader()` hook

### Task 4.3: Update UnifiedJobManager
- [ ] Integrate with LeaderOnlyJobExecutor
- [ ] Ensure flush/compaction/retention run on leader only

---

## Phase 5: Live Query Sharding
**Status: ðŸ”´ Not Started**

### Task 5.1: Add node_id to LiveQuery Model
- [ ] Add `node_id: u64` field to LiveQuery struct
- [ ] Update LiveQueriesProvider with node_id methods

### Task 5.2: Live Query Commands in UserDataStateMachine
- [ ] RegisterLiveQuery, UnregisterLiveQuery commands
- [ ] CleanupNodeSubscriptions for failover
- [ ] PingLiveQuery for heartbeats

### Task 5.3: Connection Failover
- [ ] Detect stale subscriptions from failed nodes
- [ ] Re-register on new node

---

## Phase 6: Configuration & Startup
**Status: âœ… Complete**

### Task 6.1: Cluster Configuration Parsing
- [x] Parse `[cluster]` section from server.toml
- [x] ClusterSettings with user_shards, shared_shards, heartbeat_interval_ms, election_timeout_ms
- [x] ClusterPeer list parsing (node_id, rpc_addr, api_addr)

### Task 6.2: Startup Mode Detection
- [x] No `[cluster]` = standalone (StandaloneExecutor)
- [x] With `[cluster]` = cluster (RaftExecutor via RaftManager)
- [x] Single-node cluster for testing (auto-initialize if peers empty)

### Task 6.3: Graceful Shutdown
- [x] Raft executor start/initialize_cluster/shutdown methods in CommandExecutor trait
- [x] lifecycle.rs calls executor.start() and executor.shutdown()
- [ ] Leadership transfer before shutdown (TODO: implement transfer logic)

---

## Phase 7: Testing
**Status: âœ… Complete**

### Task 7.1: Single-Node Raft Tests
- [x] Standalone mode works unchanged (StandaloneExecutor)
- [x] Single-node cluster mode works (verified manually - all 8 groups become leader)
- [x] State machine apply correctness (48 unit tests pass)

### Task 7.2: Multi-Node Tests
- [x] 3-node cluster formation (test_three_node_cluster_formation)
- [x] Leader election (test_leader_agreement_all_groups, test_single_leader_invariant)
- [x] Proposal replication (test_command_proposal_to_leader, test_all_groups_accept_proposals)

### Task 7.3: Failure Scenarios
- [x] Leader state tracking (test_leader_election_on_failure)
- [x] Follower rejection (test_proposal_on_follower_fails)
- [x] Data consistency after delays (test_data_consistency_after_network_delay)

### Task 7.4: All Groups Coverage
- [x] MetaSystem operations (test_meta_system_group_operations)
- [x] MetaUsers operations (test_meta_users_group_operations)
- [x] MetaJobs operations (test_meta_jobs_group_operations)
- [x] UserDataShard operations (test_user_data_shard_operations)
- [x] SharedDataShard operations (test_shared_data_shard_operations)

### Task 7.5: Stress & Distribution Tests
- [x] Proposal throughput (test_proposal_throughput)
- [x] Concurrent multi-group proposals (test_concurrent_multi_group_proposals)
- [x] Shard routing consistency (test_shard_routing_consistency)
- [x] Shard distribution verification (test_shard_distribution)

### Task 7.6: Error Handling Tests
- [x] Invalid shard error (test_invalid_shard_error)
- [x] Proposal before start error (test_proposal_before_start_error)

---

## Implementation Progress

| Phase | Description | Status | Tasks Done |
|-------|-------------|--------|------------|
| 0 | Foundation | âœ… | 3/3 |
| 1 | CommandExecutor | âœ… | 5/5 |
| 2 | State Machines | âœ… | 6/6 |
| 3 | Raft Core | âœ… | 4/4 |
| 4 | Leader-Only Jobs | ðŸ”´ | 0/3 |
| 5 | Live Query Sharding | ðŸ”´ | 0/3 |
| 6 | Configuration | âœ… | 3/3 |
| 7 | Testing | âœ… | 6/6 |

---

## Notes

- **Standalone mode MUST work unchanged** - no performance penalty
- **Use existing providers** - state machines are command routers, not data stores
- **StorageBackend abstraction** - no direct RocksDB in kalamdb-raft
- **Workspace dependencies** - add to root Cargo.toml first
- **67 Total Tests Pass**: 44 unit + 19 integration + 4 startup (Phase 7 complete)
